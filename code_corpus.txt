def clone_repository(repo_url, target_dir="temp_repo"):
    try:
        if os.path.exists(target_dir):
            logger.info(f"Removing existing directory: {target_dir}")
            shutil.rmtree(target_dir)
        logger.info(f"Cloning repository from {repo_url} to {target_dir}")
        Repo.clone_from(repo_url, target_dir)
        logger.info("Repository cloned successfully")
        return target_dir
    except GitCommandError as e:
        logger.error(f"Error cloning repository: {e}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise

---CODE_SEPARATOR---

def list_files_by_extension(repo_dir, extensions):
    result = {ext: [] for ext in extensions}
    for root, _, files in os.walk(repo_dir):
        if any(part.startswith(".") for part in root.split(os.sep)):
            continue
        for file in files:
            for ext in extensions:
                if file.endswith(ext):
                    result[ext].append(os.path.join(root, file))
    return result

---CODE_SEPARATOR---

def __init__(self):
        try:
            logger.info("Initializing CodeBERT analyzer...")
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"Using device: {self.device}")
            model_name = "microsoft/codebert-base"
            self.tokenizer = RobertaTokenizer.from_pretrained(model_name)
            self.model = RobertaForSequenceClassification.from_pretrained(model_name)
            self.model.to(self.device)
            self.nlp = pipeline('feature-extraction', model=model_name, tokenizer=model_name, device=0 if torch.cuda.is_available() else -1)
            logger.info("CodeBERT initialization complete")
        except Exception as e:
            logger.error(f"Error initializing CodeBERT: {e}")
            raise
    def code_quality_score(self, code):
        try:
            inputs = self.tokenizer(code, return_tensors="pt", truncation=True, max_length=512).to(self.device)
            outputs = self.model(**inputs)
            embeddings = outputs.logits
            quality_score = torch.sigmoid(torch.mean(embeddings)).item()
            return quality_score
        except Exception as e:
            logger.error(f"Error calculating code quality score: {e}")
            return 0.5  
    def suggest_improvements(self, code, file_ext):
        suggestions = []
        if len(code) > 10000:
            suggestions.append("Code is too large for detailed analysis. Consider breaking it into smaller modules.")
            return suggestions
        if not code.strip():
            suggestions.append("Code file is empty or contains only whitespace.")
            return suggestions
        try:
            lines = code.split('\n')
            function_start_patterns = {
                '.py': ['def ', 'async def '],
                '.js': ['function ', '=>', 'async function'],
                '.jsx': ['function ', '=>', 'async function'],
                '.ts': ['function ', '=>', 'async function'],
                '.tsx': ['function ', '=>', 'async function']
            }
            current_function_lines = 0
            in_function = False
            for line in lines:
                line = line.strip()
                for pattern in function_start_patterns.get(file_ext, []):
                    if pattern in line:
                        in_function = True
                        current_function_lines = 0
                if in_function:
                    current_function_lines += 1
                if in_function and ('}' in line or line.startswith('return')):
                    if current_function_lines > 50:
                        suggestions.append(f"Consider breaking down large function with {current_function_lines} lines into smaller, more focused functions.")
                    in_function = False
            if file_ext in ['.py', '.js', '.jsx', '.ts', '.tsx']:
                string_literal_count = len([line for line in lines if '"' in line or "'" in line])
                if string_literal_count > 10:
                    suggestions.append("Consider extracting hardcoded string literals into constants or configuration files.")
            indentation_levels = []
            for line in lines:
                if line.strip():  
                    leading_spaces = len(line) - len(line.lstrip())
                    indentation_levels.append(leading_spaces)
            if indentation_levels and max(indentation_levels) > 24:  
                suggestions.append("Deep nesting detected. Consider refactoring to reduce complexity and improve readability.")
            cleaned_lines = [line.strip() for line in lines if line.strip() and not line.strip().startswith('#') and not line.strip().startswith('//')]
            seen_patterns = {}
            for i in range(len(cleaned_lines) - 3):
                pattern = '\n'.join(cleaned_lines[i:i+3])
                if pattern in seen_patterns:
                    seen_patterns[pattern] += 1
                else:
                    seen_patterns[pattern] = 1
            duplicate_patterns = [pattern for pattern, count in seen_patterns.items() if count > 1]
            if duplicate_patterns:
                suggestions.append(f"Detected {len(duplicate_patterns)} potentially duplicated code patterns. Consider refactoring into reusable functions.")
            quality_score = self.code_quality_score(code)
            if quality_score < 0.4:
                suggestions.append("Overall code quality appears to be low. Consider refactoring using clean code principles.")
            elif quality_score < 0.6:
                suggestions.append("Code quality is average. Consider adding more documentation and improving naming conventions.")
            if file_ext == '.py':
                if 'import *' in code:
                    suggestions.append("Avoid using 'import *' as it can lead to namespace pollution. Import only what you need.")
                if 'except:' in code and 'except Exception:' not in code:
                    suggestions.append("Avoid bare 'except:' clauses. Specify the exceptions you want to catch.")
                if 'global ' in code:
                    suggestions.append("Consider avoiding global variables as they can lead to maintainability issues.")
            elif file_ext in ['.js', '.jsx', '.ts', '.tsx']:
                if 'var ' in code:
                    suggestions.append("Consider using 'const' or 'let' instead of 'var' for better scoping behavior.")
                if '==' in code and '===' not in code:
                    suggestions.append("Consider using '===' instead of '==' for strict equality comparisons.")
                if 'setTimeout(' in code and 'clearTimeout(' not in code:
                    suggestions.append("Make sure to clear timeouts to prevent memory leaks, especially in React components.")
            return suggestions
        except Exception as e:
            logger.error(f"Error suggesting improvements: {e}")
            suggestions.append("Error analyzing code. Try again with a smaller code snippet.")
            return suggestions
    def analyze_repository(self, repo_path, graph):
        results = {
            "file_analysis": {},
            "most_complex_files": [],
            "overall_suggestions": []
        }
        file_extensions = {
            '.py': 'Python',
            '.js': 'JavaScript',
            '.jsx': 'React JSX',
            '.ts': 'TypeScript',
            '.tsx': 'React TypeScript'
        }
        files_to_analyze = []
        for node in graph.nodes():
            node_type = graph.nodes[node].get('type')
            if node_type in ['python_file', 'js_file']:
                file_path = os.path.join(repo_path, node)
                if os.path.exists(file_path) and os.path.getsize(file_path) < 1000000:  
                    _, ext = os.path.splitext(file_path)
                    if ext in file_extensions:
                        files_to_analyze.append((file_path, ext))
        logger.info(f"Analyzing {len(files_to_analyze)} files with CodeBERT...")
        complexity_scores = {}
        for file_path, ext in tqdm(files_to_analyze, desc="Analyzing files"):
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                    code = file.read()
                if not code.strip():
                    continue
                quality_score = self.code_quality_score(code)
                suggestions = self.suggest_improvements(code, ext)
                complexity_scores[file_path] = 1.0 - quality_score  
                loc = len(code.split('\n'))
                comment_lines = len([line for line in code.split('\n') if line.strip().startswith('#') or line.strip().startswith('//')])
                rel_path = os.path.relpath(file_path, repo_path)
                results["file_analysis"][rel_path] = {
                    "quality_score": quality_score,
                    "language": file_extensions[ext],
                    "lines_of_code": loc,
                    "comment_lines": comment_lines,
                    "comment_ratio": comment_lines / loc if loc > 0 else 0,
                    "suggestions": suggestions
                }
            except Exception as e:
                logger.error(f"Error analyzing file {file_path}: {e}")
        most_complex = sorted(complexity_scores.items(), key=lambda x: x[1], reverse=True)[:5]
        results["most_complex_files"] = [os.path.relpath(file_path, repo_path) for file_path, _ in most_complex]
        if results["file_analysis"]:
            avg_quality = sum(file["quality_score"] for file in results["file_analysis"].values()) / len(results["file_analysis"])
            avg_comment_ratio = sum(file["comment_ratio"] for file in results["file_analysis"].values()) / len(results["file_analysis"])
            if avg_quality < 0.5:
                results["overall_suggestions"].append("The overall code quality appears to be below average. Consider implementing a code review process.")
            if avg_comment_ratio < 0.1:
                results["overall_suggestions"].append("The codebase has a low comment ratio. Consider improving documentation to enhance maintainability.")
            quality_variance = sum((file["quality_score"] - avg_quality) ** 2 for file in results["file_analysis"].values())
            if quality_variance > 0.05:
                results["overall_suggestions"].append("Code quality varies significantly across files. Consider implementing coding standards and linters.")
        return results

---CODE_SEPARATOR---

class CodeBERTAnalyzer:
    def __init__(self):
        try:
            logger.info("Initializing CodeBERT analyzer...")
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            logger.info(f"Using device: {self.device}")
            model_name = "microsoft/codebert-base"
            self.tokenizer = RobertaTokenizer.from_pretrained(model_name)
            self.model = RobertaForSequenceClassification.from_pretrained(model_name)
            self.model.to(self.device)
            self.nlp = pipeline('feature-extraction', model=model_name, tokenizer=model_name, device=0 if torch.cuda.is_available() else -1)
            logger.info("CodeBERT initialization complete")
        except Exception as e:
            logger.error(f"Error initializing CodeBERT: {e}")
            raise
    def code_quality_score(self, code):
        try:
            inputs = self.tokenizer(code, return_tensors="pt", truncation=True, max_length=512).to(self.device)
            outputs = self.model(**inputs)
            embeddings = outputs.logits
            quality_score = torch.sigmoid(torch.mean(embeddings)).item()
            return quality_score
        except Exception as e:
            logger.error(f"Error calculating code quality score: {e}")
            return 0.5  
    def suggest_improvements(self, code, file_ext):
        suggestions = []
        if len(code) > 10000:
            suggestions.append("Code is too large for detailed analysis. Consider breaking it into smaller modules.")
            return suggestions
        if not code.strip():
            suggestions.append("Code file is empty or contains only whitespace.")
            return suggestions
        try:
            lines = code.split('\n')
            function_start_patterns = {
                '.py': ['def ', 'async def '],
                '.js': ['function ', '=>', 'async function'],
                '.jsx': ['function ', '=>', 'async function'],
                '.ts': ['function ', '=>', 'async function'],
                '.tsx': ['function ', '=>', 'async function']
            }
            current_function_lines = 0
            in_function = False
            for line in lines:
                line = line.strip()
                for pattern in function_start_patterns.get(file_ext, []):
                    if pattern in line:
                        in_function = True
                        current_function_lines = 0
                if in_function:
                    current_function_lines += 1
                if in_function and ('}' in line or line.startswith('return')):
                    if current_function_lines > 50:
                        suggestions.append(f"Consider breaking down large function with {current_function_lines} lines into smaller, more focused functions.")
                    in_function = False
            if file_ext in ['.py', '.js', '.jsx', '.ts', '.tsx']:
                string_literal_count = len([line for line in lines if '"' in line or "'" in line])
                if string_literal_count > 10:
                    suggestions.append("Consider extracting hardcoded string literals into constants or configuration files.")
            indentation_levels = []
            for line in lines:
                if line.strip():  
                    leading_spaces = len(line) - len(line.lstrip())
                    indentation_levels.append(leading_spaces)
            if indentation_levels and max(indentation_levels) > 24:  
                suggestions.append("Deep nesting detected. Consider refactoring to reduce complexity and improve readability.")
            cleaned_lines = [line.strip() for line in lines if line.strip() and not line.strip().startswith('#') and not line.strip().startswith('//')]
            seen_patterns = {}
            for i in range(len(cleaned_lines) - 3):
                pattern = '\n'.join(cleaned_lines[i:i+3])
                if pattern in seen_patterns:
                    seen_patterns[pattern] += 1
                else:
                    seen_patterns[pattern] = 1
            duplicate_patterns = [pattern for pattern, count in seen_patterns.items() if count > 1]
            if duplicate_patterns:
                suggestions.append(f"Detected {len(duplicate_patterns)} potentially duplicated code patterns. Consider refactoring into reusable functions.")
            quality_score = self.code_quality_score(code)
            if quality_score < 0.4:
                suggestions.append("Overall code quality appears to be low. Consider refactoring using clean code principles.")
            elif quality_score < 0.6:
                suggestions.append("Code quality is average. Consider adding more documentation and improving naming conventions.")
            if file_ext == '.py':
                if 'import *' in code:
                    suggestions.append("Avoid using 'import *' as it can lead to namespace pollution. Import only what you need.")
                if 'except:' in code and 'except Exception:' not in code:
                    suggestions.append("Avoid bare 'except:' clauses. Specify the exceptions you want to catch.")
                if 'global ' in code:
                    suggestions.append("Consider avoiding global variables as they can lead to maintainability issues.")
            elif file_ext in ['.js', '.jsx', '.ts', '.tsx']:
                if 'var ' in code:
                    suggestions.append("Consider using 'const' or 'let' instead of 'var' for better scoping behavior.")
                if '==' in code and '===' not in code:
                    suggestions.append("Consider using '===' instead of '==' for strict equality comparisons.")
                if 'setTimeout(' in code and 'clearTimeout(' not in code:
                    suggestions.append("Make sure to clear timeouts to prevent memory leaks, especially in React components.")
            return suggestions
        except Exception as e:
            logger.error(f"Error suggesting improvements: {e}")
            suggestions.append("Error analyzing code. Try again with a smaller code snippet.")
            return suggestions
    def analyze_repository(self, repo_path, graph):
        results = {
            "file_analysis": {},
            "most_complex_files": [],
            "overall_suggestions": []
        }
        file_extensions = {
            '.py': 'Python',
            '.js': 'JavaScript',
            '.jsx': 'React JSX',
            '.ts': 'TypeScript',
            '.tsx': 'React TypeScript'
        }
        files_to_analyze = []
        for node in graph.nodes():
            node_type = graph.nodes[node].get('type')
            if node_type in ['python_file', 'js_file']:
                file_path = os.path.join(repo_path, node)
                if os.path.exists(file_path) and os.path.getsize(file_path) < 1000000:  
                    _, ext = os.path.splitext(file_path)
                    if ext in file_extensions:
                        files_to_analyze.append((file_path, ext))
        logger.info(f"Analyzing {len(files_to_analyze)} files with CodeBERT...")
        complexity_scores = {}
        for file_path, ext in tqdm(files_to_analyze, desc="Analyzing files"):
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                    code = file.read()
                if not code.strip():
                    continue
                quality_score = self.code_quality_score(code)
                suggestions = self.suggest_improvements(code, ext)
                complexity_scores[file_path] = 1.0 - quality_score  
                loc = len(code.split('\n'))
                comment_lines = len([line for line in code.split('\n') if line.strip().startswith('#') or line.strip().startswith('//')])
                rel_path = os.path.relpath(file_path, repo_path)
                results["file_analysis"][rel_path] = {
                    "quality_score": quality_score,
                    "language": file_extensions[ext],
                    "lines_of_code": loc,
                    "comment_lines": comment_lines,
                    "comment_ratio": comment_lines / loc if loc > 0 else 0,
                    "suggestions": suggestions
                }
            except Exception as e:
                logger.error(f"Error analyzing file {file_path}: {e}")
        most_complex = sorted(complexity_scores.items(), key=lambda x: x[1], reverse=True)[:5]
        results["most_complex_files"] = [os.path.relpath(file_path, repo_path) for file_path, _ in most_complex]
        if results["file_analysis"]:
            avg_quality = sum(file["quality_score"] for file in results["file_analysis"].values()) / len(results["file_analysis"])
            avg_comment_ratio = sum(file["comment_ratio"] for file in results["file_analysis"].values()) / len(results["file_analysis"])
            if avg_quality < 0.5:
                results["overall_suggestions"].append("The overall code quality appears to be below average. Consider implementing a code review process.")
            if avg_comment_ratio < 0.1:
                results["overall_suggestions"].append("The codebase has a low comment ratio. Consider improving documentation to enhance maintainability.")
            quality_variance = sum((file["quality_score"] - avg_quality) ** 2 for file in results["file_analysis"].values())
            if quality_variance > 0.05:
                results["overall_suggestions"].append("Code quality varies significantly across files. Consider implementing coding standards and linters.")
        return results

---CODE_SEPARATOR---

def __init__(self, repo_path):
        self.repo_path = repo_path
    def parse_python_imports(self, file_path):
        imports = []
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            try:
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for name in node.names:
                            imports.append(name.name)
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            module = node.module.split('.')[0]
                            imports.append(module)
            except SyntaxError:
                try:
                    tree = astroid.parse(content)
                    for node in tree.nodes_of_class((astroid.Import, astroid.ImportFrom)):
                        if isinstance(node, astroid.Import):
                            for name in node.names:
                                imports.append(name[0])
                        elif isinstance(node, astroid.ImportFrom):
                            if node.modname:
                                module = node.modname.split('.')[0]
                                imports.append(module)
                except Exception as e:
                    logger.warning(f"Astroid parsing failed for {file_path}: {e}")
                    import_regex = r'^\s*(?:from|import)\s+([a-zA-Z0-9_\.]+)'
                    for line in content.split('\n'):
                        match = re.match(import_regex, line)
                        if match:
                            module = match.group(1).split('.')[0]
                            imports.append(module)
            return list(set(imports))  
        except Exception as e:
            logger.error(f"Error parsing Python imports from {file_path}: {e}")
            return []
    def parse_js_imports(self, file_path):
        imports = []
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            try:
                tree = esprima.parseModule(content)
                def extract_imports(node):
                    if node.type == 'ImportDeclaration':
                        source = node.source.value
                        imports.append(source)
                    elif node.type == 'CallExpression' and node.callee.name == 'require':
                        if node.arguments and node.arguments[0].type == 'Literal':
                            imports.append(node.arguments[0].value)
                    for key, value in node.items():
                        if isinstance(value, dict) and 'type' in value:
                            extract_imports(value)
                        elif isinstance(value, list):
                            for item in value:
                                if isinstance(item, dict) and 'type' in item:
                                    extract_imports(item)
                extract_imports(tree.toDict())
            except Exception as e:
                logger.warning(f"Esprima parsing failed for {file_path}: {e}")
                es6_import_regex = r'import\s+(?:.*?\s+from\s+)?[\'"]([^\'"]*)[\'"]\s*;?'
                imports.extend(re.findall(es6_import_regex, content))
                require_regex = r'require\([\'"]([^\'"]*)[\'"]'
                imports.extend(re.findall(require_regex, content))
            cleaned_imports = []
            for imp in imports:
                if imp.startswith('.'):
                    continue  
                if '/' in imp and not imp.startswith('@'):
                    imp = imp.split('/')[0]
                elif imp.startswith('@'):
                    parts = imp.split('/')
                    if len(parts) > 1:
                        imp = f"{parts[0]}/{parts[1]}"
                cleaned_imports.append(imp)
            return list(set(cleaned_imports))  
        except Exception as e:
            logger.error(f"Error parsing JS imports from {file_path}: {e}")
            return []
    def parse_package_json(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
            dependencies = {}
            if 'dependencies' in data:
                dependencies.update(data['dependencies'])
            if 'devDependencies' in data:
                dependencies.update(data['devDependencies'])
            return dependencies
        except Exception as e:
            logger.error(f"Error parsing package.json from {file_path}: {e}")
            return {}
    def parse_requirements_txt(self, file_path):
        dependencies = {}
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                for line in file:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    if 'git+' in line:
                        parts = line.split('/')
                        if len(parts) > 1:
                            package = parts[-1].split('@')[0].split('.git')[0]
                            dependencies[package] = 'git'
                        continue
                    for spec in ['==', '>=', '<=', '~=', '>', '<', '=']:
                        if spec in line:
                            package, version = line.split(spec, 1)
                            dependencies[package.strip()] = version.strip()
                            break
                    else:
                        dependencies[line] = 'latest'
            return dependencies
        except Exception as e:
            logger.error(f"Error parsing requirements.txt from {file_path}: {e}")
            return {}
    def extract_all_dependencies(self):
        dependencies = {
            'python': {
                'files': {},
                'requirements': {}
            },
            'javascript': {
                'files': {},
                'package_json': {}
            }
        }
        for root, _, files in os.walk(self.repo_path):
            if any(part.startswith(".") for part in root.split(os.sep)):
                continue
            for file in files:
                file_path = os.path.join(root, file)
                if file.endswith('.py'):
                    imports = self.parse_python_imports(file_path)
                    if imports:
                        rel_path = os.path.relpath(file_path, self.repo_path)
                        dependencies['python']['files'][rel_path] = imports
                elif file.endswith(('.js', '.jsx', '.ts', '.tsx')):
                    imports = self.parse_js_imports(file_path)
                    if imports:
                        rel_path = os.path.relpath(file_path, self.repo_path)
                        dependencies['javascript']['files'][rel_path] = imports
                elif file == 'requirements.txt':
                    dependencies['python']['requirements'] = self.parse_requirements_txt(file_path)
                elif file == 'package.json':
                    dependencies['javascript']['package_json'] = self.parse_package_json(file_path)
        return dependencies

---CODE_SEPARATOR---

class DependencyParser:
    def __init__(self, repo_path):
        self.repo_path = repo_path
    def parse_python_imports(self, file_path):
        imports = []
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            try:
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for name in node.names:
                            imports.append(name.name)
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            module = node.module.split('.')[0]
                            imports.append(module)
            except SyntaxError:
                try:
                    tree = astroid.parse(content)
                    for node in tree.nodes_of_class((astroid.Import, astroid.ImportFrom)):
                        if isinstance(node, astroid.Import):
                            for name in node.names:
                                imports.append(name[0])
                        elif isinstance(node, astroid.ImportFrom):
                            if node.modname:
                                module = node.modname.split('.')[0]
                                imports.append(module)
                except Exception as e:
                    logger.warning(f"Astroid parsing failed for {file_path}: {e}")
                    import_regex = r'^\s*(?:from|import)\s+([a-zA-Z0-9_\.]+)'
                    for line in content.split('\n'):
                        match = re.match(import_regex, line)
                        if match:
                            module = match.group(1).split('.')[0]
                            imports.append(module)
            return list(set(imports))  
        except Exception as e:
            logger.error(f"Error parsing Python imports from {file_path}: {e}")
            return []
    def parse_js_imports(self, file_path):
        imports = []
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            try:
                tree = esprima.parseModule(content)
                def extract_imports(node):
                    if node.type == 'ImportDeclaration':
                        source = node.source.value
                        imports.append(source)
                    elif node.type == 'CallExpression' and node.callee.name == 'require':
                        if node.arguments and node.arguments[0].type == 'Literal':
                            imports.append(node.arguments[0].value)
                    for key, value in node.items():
                        if isinstance(value, dict) and 'type' in value:
                            extract_imports(value)
                        elif isinstance(value, list):
                            for item in value:
                                if isinstance(item, dict) and 'type' in item:
                                    extract_imports(item)
                extract_imports(tree.toDict())
            except Exception as e:
                logger.warning(f"Esprima parsing failed for {file_path}: {e}")
                es6_import_regex = r'import\s+(?:.*?\s+from\s+)?[\'"]([^\'"]*)[\'"]\s*;?'
                imports.extend(re.findall(es6_import_regex, content))
                require_regex = r'require\([\'"]([^\'"]*)[\'"]'
                imports.extend(re.findall(require_regex, content))
            cleaned_imports = []
            for imp in imports:
                if imp.startswith('.'):
                    continue  
                if '/' in imp and not imp.startswith('@'):
                    imp = imp.split('/')[0]
                elif imp.startswith('@'):
                    parts = imp.split('/')
                    if len(parts) > 1:
                        imp = f"{parts[0]}/{parts[1]}"
                cleaned_imports.append(imp)
            return list(set(cleaned_imports))  
        except Exception as e:
            logger.error(f"Error parsing JS imports from {file_path}: {e}")
            return []
    def parse_package_json(self, file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
            dependencies = {}
            if 'dependencies' in data:
                dependencies.update(data['dependencies'])
            if 'devDependencies' in data:
                dependencies.update(data['devDependencies'])
            return dependencies
        except Exception as e:
            logger.error(f"Error parsing package.json from {file_path}: {e}")
            return {}
    def parse_requirements_txt(self, file_path):
        dependencies = {}
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                for line in file:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    if 'git+' in line:
                        parts = line.split('/')
                        if len(parts) > 1:
                            package = parts[-1].split('@')[0].split('.git')[0]
                            dependencies[package] = 'git'
                        continue
                    for spec in ['==', '>=', '<=', '~=', '>', '<', '=']:
                        if spec in line:
                            package, version = line.split(spec, 1)
                            dependencies[package.strip()] = version.strip()
                            break
                    else:
                        dependencies[line] = 'latest'
            return dependencies
        except Exception as e:
            logger.error(f"Error parsing requirements.txt from {file_path}: {e}")
            return {}
    def extract_all_dependencies(self):
        dependencies = {
            'python': {
                'files': {},
                'requirements': {}
            },
            'javascript': {
                'files': {},
                'package_json': {}
            }
        }
        for root, _, files in os.walk(self.repo_path):
            if any(part.startswith(".") for part in root.split(os.sep)):
                continue
            for file in files:
                file_path = os.path.join(root, file)
                if file.endswith('.py'):
                    imports = self.parse_python_imports(file_path)
                    if imports:
                        rel_path = os.path.relpath(file_path, self.repo_path)
                        dependencies['python']['files'][rel_path] = imports
                elif file.endswith(('.js', '.jsx', '.ts', '.tsx')):
                    imports = self.parse_js_imports(file_path)
                    if imports:
                        rel_path = os.path.relpath(file_path, self.repo_path)
                        dependencies['javascript']['files'][rel_path] = imports
                elif file == 'requirements.txt':
                    dependencies['python']['requirements'] = self.parse_requirements_txt(file_path)
                elif file == 'package.json':
                    dependencies['javascript']['package_json'] = self.parse_package_json(file_path)
        return dependencies

---CODE_SEPARATOR---

def __init__(self, dependencies):
        self.dependencies = dependencies
        self.graph = nx.DiGraph()
    def build_graph(self):
        logger.info("Building dependency graph...")
        python_files = self.dependencies['python']['files']
        for file_path, imports in python_files.items():
            self.graph.add_node(file_path, 
                               type='python_file', 
                               label=file_path,
                               title=f"Python: {file_path}")
            for module in imports:
                if module in python_files:
                    self.graph.add_edge(file_path, module, type='internal_import')
                else:
                    module_node = f"py_pkg:{module}"
                    if not self.graph.has_node(module_node):
                        version = self.dependencies['python']['requirements'].get(module, 'unknown')
                        self.graph.add_node(module_node, 
                                          type='python_package', 
                                          label=module,
                                          title=f"Python Package: {module} ({version})")
                    self.graph.add_edge(file_path, module_node, type='external_import')
        js_files = self.dependencies['javascript']['files']
        for file_path, imports in js_files.items():
            self.graph.add_node(file_path, 
                               type='js_file', 
                               label=file_path,
                               title=f"JavaScript: {file_path}")
            for module in imports:
                if module in js_files:
                    self.graph.add_edge(file_path, module, type='internal_import')
                else:
                    module_node = f"js_pkg:{module}"
                    if not self.graph.has_node(module_node):
                        version = self.dependencies['javascript']['package_json'].get(module, 'unknown')
                        self.graph.add_node(module_node, 
                                          type='js_package', 
                                          label=module,
                                          title=f"JS Package: {module} ({version})")
                    self.graph.add_edge(file_path, module_node, type='external_import')
        self.graph.graph['python_files'] = len(python_files)
        self.graph.graph['js_files'] = len(js_files)
        self.graph.graph['python_packages'] = len(self.dependencies['python']['requirements'])
        self.graph.graph['js_packages'] = len(self.dependencies['javascript']['package_json'])
        logger.info(f"Graph built with {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges")
        return self.graph
    def get_central_nodes(self, top_n=5):
        if self.graph.number_of_nodes() == 0:
            return {}
        centrality = {}
        try:
            degree_centrality = nx.degree_centrality(self.graph)
            centrality['degree'] = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]
        except Exception as e:
            logger.warning(f"Error calculating degree centrality: {e}")
        try:
            betweenness_centrality = nx.betweenness_centrality(self.graph)
            centrality['betweenness'] = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]
        except Exception as e:
            logger.warning(f"Error calculating betweenness centrality: {e}")
        try:
            pagerank = nx.pagerank(self.graph)
            centrality['pagerank'] = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:top_n]
        except Exception as e:
            logger.warning(f"Error calculating PageRank: {e}")
        return centrality
    def identify_clusters(self):
        try:
            undirected_graph = self.graph.to_undirected()
            try:
                import community as community_louvain
                partition = community_louvain.best_partition(undirected_graph)
                return partition
            except ImportError:
                logger.warning("Community detection package not found, using connected components")
                components = list(nx.connected_components(undirected_graph))
                partition = {}
                for i, component in enumerate(components):
                    for node in component:
                        partition[node] = i
                return partition
        except Exception as e:
            logger.warning(f"Error identifying clusters: {e}")
            return {}
    def analyze_dependencies(self):
        if self.graph.number_of_nodes() == 0:
            return {
                "error": "Empty graph, no dependencies to analyze"
            }
        analysis = {
            "stats": {
                "total_nodes": self.graph.number_of_nodes(),
                "total_edges": self.graph.number_of_edges(),
                "python_files": self.graph.graph.get('python_files', 0),
                "js_files": self.graph.graph.get('js_files', 0),
                "python_packages": self.graph.graph.get('python_packages', 0),
                "js_packages": self.graph.graph.get('js_packages', 0),
            },
            "central_nodes": self.get_central_nodes(),
            "clusters": len(set(self.identify_clusters().values())) if self.identify_clusters() else 0,
            "density": nx.density(self.graph),
            "avg_clustering": nx.average_clustering(self.graph.to_undirected()),
            "recommendations": []
        }
        if analysis["density"] > 0.7:
            analysis["recommendations"].append("High graph density suggests tight coupling. Consider modularizing the codebase.")
        if analysis["avg_clustering"] < 0.2:
            analysis["recommendations"].append("Low clustering coefficient suggests poor code organization. Consider grouping related functionality.")
        try:
            cycles = list(nx.simple_cycles(self.graph))
            if cycles:
                analysis["recommendations"].append(f"Found {len(cycles)} circular dependencies. Consider refactoring to remove cycles.")
                analysis["cycles"] = [cycle for cycle in cycles[:5]]  
        except Exception as e:
            logger.warning(f"Error detecting cycles: {e}")
        orphans = [node for node in self.graph.nodes() if self.graph.degree(node) == 0]
        if orphans:
            analysis["recommendations"].append(f"Found {len(orphans)} orphaned files that aren't connected to the dependency graph.")
            analysis["orphans"] = orphans[:10]  
        return analysis

---CODE_SEPARATOR---

class DependencyGraphBuilder:
    def __init__(self, dependencies):
        self.dependencies = dependencies
        self.graph = nx.DiGraph()
    def build_graph(self):
        logger.info("Building dependency graph...")
        python_files = self.dependencies['python']['files']
        for file_path, imports in python_files.items():
            self.graph.add_node(file_path, 
                               type='python_file', 
                               label=file_path,
                               title=f"Python: {file_path}")
            for module in imports:
                if module in python_files:
                    self.graph.add_edge(file_path, module, type='internal_import')
                else:
                    module_node = f"py_pkg:{module}"
                    if not self.graph.has_node(module_node):
                        version = self.dependencies['python']['requirements'].get(module, 'unknown')
                        self.graph.add_node(module_node, 
                                          type='python_package', 
                                          label=module,
                                          title=f"Python Package: {module} ({version})")
                    self.graph.add_edge(file_path, module_node, type='external_import')
        js_files = self.dependencies['javascript']['files']
        for file_path, imports in js_files.items():
            self.graph.add_node(file_path, 
                               type='js_file', 
                               label=file_path,
                               title=f"JavaScript: {file_path}")
            for module in imports:
                if module in js_files:
                    self.graph.add_edge(file_path, module, type='internal_import')
                else:
                    module_node = f"js_pkg:{module}"
                    if not self.graph.has_node(module_node):
                        version = self.dependencies['javascript']['package_json'].get(module, 'unknown')
                        self.graph.add_node(module_node, 
                                          type='js_package', 
                                          label=module,
                                          title=f"JS Package: {module} ({version})")
                    self.graph.add_edge(file_path, module_node, type='external_import')
        self.graph.graph['python_files'] = len(python_files)
        self.graph.graph['js_files'] = len(js_files)
        self.graph.graph['python_packages'] = len(self.dependencies['python']['requirements'])
        self.graph.graph['js_packages'] = len(self.dependencies['javascript']['package_json'])
        logger.info(f"Graph built with {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges")
        return self.graph
    def get_central_nodes(self, top_n=5):
        if self.graph.number_of_nodes() == 0:
            return {}
        centrality = {}
        try:
            degree_centrality = nx.degree_centrality(self.graph)
            centrality['degree'] = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]
        except Exception as e:
            logger.warning(f"Error calculating degree centrality: {e}")
        try:
            betweenness_centrality = nx.betweenness_centrality(self.graph)
            centrality['betweenness'] = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]
        except Exception as e:
            logger.warning(f"Error calculating betweenness centrality: {e}")
        try:
            pagerank = nx.pagerank(self.graph)
            centrality['pagerank'] = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:top_n]
        except Exception as e:
            logger.warning(f"Error calculating PageRank: {e}")
        return centrality
    def identify_clusters(self):
        try:
            undirected_graph = self.graph.to_undirected()
            try:
                import community as community_louvain
                partition = community_louvain.best_partition(undirected_graph)
                return partition
            except ImportError:
                logger.warning("Community detection package not found, using connected components")
                components = list(nx.connected_components(undirected_graph))
                partition = {}
                for i, component in enumerate(components):
                    for node in component:
                        partition[node] = i
                return partition
        except Exception as e:
            logger.warning(f"Error identifying clusters: {e}")
            return {}
    def analyze_dependencies(self):
        if self.graph.number_of_nodes() == 0:
            return {
                "error": "Empty graph, no dependencies to analyze"
            }
        analysis = {
            "stats": {
                "total_nodes": self.graph.number_of_nodes(),
                "total_edges": self.graph.number_of_edges(),
                "python_files": self.graph.graph.get('python_files', 0),
                "js_files": self.graph.graph.get('js_files', 0),
                "python_packages": self.graph.graph.get('python_packages', 0),
                "js_packages": self.graph.graph.get('js_packages', 0),
            },
            "central_nodes": self.get_central_nodes(),
            "clusters": len(set(self.identify_clusters().values())) if self.identify_clusters() else 0,
            "density": nx.density(self.graph),
            "avg_clustering": nx.average_clustering(self.graph.to_undirected()),
            "recommendations": []
        }
        if analysis["density"] > 0.7:
            analysis["recommendations"].append("High graph density suggests tight coupling. Consider modularizing the codebase.")
        if analysis["avg_clustering"] < 0.2:
            analysis["recommendations"].append("Low clustering coefficient suggests poor code organization. Consider grouping related functionality.")
        try:
            cycles = list(nx.simple_cycles(self.graph))
            if cycles:
                analysis["recommendations"].append(f"Found {len(cycles)} circular dependencies. Consider refactoring to remove cycles.")
                analysis["cycles"] = [cycle for cycle in cycles[:5]]  
        except Exception as e:
            logger.warning(f"Error detecting cycles: {e}")
        orphans = [node for node in self.graph.nodes() if self.graph.degree(node) == 0]
        if orphans:
            analysis["recommendations"].append(f"Found {len(orphans)} orphaned files that aren't connected to the dependency graph.")
            analysis["orphans"] = orphans[:10]  
        return analysis

---CODE_SEPARATOR---

def get_repo_name(repo_url):
    parsed_url = urlparse(repo_url)
    path = parsed_url.path.strip('/')
    return path.split('/')[-1].replace('.git', '')

---CODE_SEPARATOR---

def parse_args():
    parser = argparse.ArgumentParser(
        description='Analyze GitHub repository dependencies and suggest improvements'
    )
    parser.add_argument('repo_url', help='URL of the GitHub repository to analyze')
    parser.add_argument('--output-dir', '-o', default='output', help='Directory to store the output files')
    parser.add_argument('--tmp-dir', '-t', default='temp_repo', help='Temporary directory for cloning the repository')
    parser.add_argument('--skip-codebert', action='store_true', help='Skip CodeBERT analysis (faster)')
    parser.add_argument('--skip-visualization', action='store_true', help='Skip visualization generation')
    parser.add_argument('--verbose', '-v', action='store_true', help='Show verbose output')
    return parser.parse_args()

---CODE_SEPARATOR---

def main():
    start_time = time.time()
    args = parse_args()
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    os.makedirs(args.output_dir, exist_ok=True)
    try:
        logger.info(f"Step 1: Cloning repository from {args.repo_url}")
        repo_dir = clone_repository(args.repo_url, args.tmp_dir)
        repo_name = get_repo_name(args.repo_url)
        logger.info(f"Repository '{repo_name}' cloned successfully")
        logger.info("Step 2: Parsing dependencies")
        parser = DependencyParser(repo_dir)
        dependencies = parser.extract_all_dependencies()
        with open(os.path.join(args.output_dir, 'dependencies.json'), 'w') as f:
            json.dump(dependencies, f, indent=2)
        logger.info(f"Found {len(dependencies['python']['files'])} Python files and "
                   f"{len(dependencies['javascript']['files'])} JavaScript files")
        logger.info("Step 3: Building dependency graph")
        graph_builder = DependencyGraphBuilder(dependencies)
        graph = graph_builder.build_graph()
        analysis_results = graph_builder.analyze_dependencies()
        with open(os.path.join(args.output_dir, 'graph_analysis.json'), 'w') as f:
            json.dump(analysis_results, f, indent=2)
        codebert_results = {}
        if not args.skip_codebert:
            logger.info("Step 4: Analyzing code quality with CodeBERT")
            try:
                analyzer = CodeBERTAnalyzer()
                codebert_results = analyzer.analyze_repository(repo_dir, graph)
                with open(os.path.join(args.output_dir, 'codebert_analysis.json'), 'w') as f:
                    json.dump(codebert_results, f, indent=2)
            except Exception as e:
                logger.error(f"CodeBERT analysis failed: {e}")
                logger.info("Continuing without CodeBERT analysis")
        else:
            logger.info("CodeBERT analysis skipped")
        if not args.skip_visualization:
            logger.info("Step 5: Generating visualizations")
            visualizer = DependencyVisualizer(graph, repo_name)
            html_path = os.path.join(args.output_dir, 'dependency_graph.html')
            visualizer.create_network_visualization(html_path)
            png_path = os.path.join(args.output_dir, 'dependency_graph.png')
            visualizer.create_static_graph(png_path)
            json_path = os.path.join(args.output_dir, 'graph_data.json')
            visualizer.export_graph_data(json_path)
        else:
            logger.info("Visualization generation skipped")
        generate_report(args.output_dir, repo_name, dependencies, analysis_results, codebert_results)
        if os.path.exists(repo_dir) and repo_dir.startswith("temp_"):
            import shutil
            logger.info(f"Cleaning up temporary directory: {repo_dir}")
            shutil.rmtree(repo_dir)
        elapsed_time = time.time() - start_time
        logger.info(f"Analysis completed in {elapsed_time:.2f} seconds")
        logger.info(f"Results saved to {os.path.abspath(args.output_dir)}")
        print("\n=== Repository Analysis Summary ===")
        print(f"Repository: {repo_name}")
        print(f"Python files: {graph.graph.get('python_files', 0)}")
        print(f"JavaScript files: {graph.graph.get('js_files', 0)}")
        print(f"Python packages: {graph.graph.get('python_packages', 0)}")
        print(f"JavaScript packages: {graph.graph.get('js_packages', 0)}")
        print(f"Total nodes in dependency graph: {graph.number_of_nodes()}")
        print(f"Total edges in dependency graph: {graph.number_of_edges()}")
        if not args.skip_visualization:
            print(f"\nHTML visualization: {os.path.abspath(html_path)}")
        print(f"\nFull report: {os.path.abspath(os.path.join(args.output_dir, 'report.md'))}")
    except Exception as e:
        logger.error(f"Error during analysis: {e}", exc_info=True)
        print(f"Error: {e}")
        return 1
    return 0

---CODE_SEPARATOR---

def generate_report(output_dir, repo_name, dependencies, graph_analysis, codebert_results):
    report_path = os.path.join(output_dir, 'report.md')
    with open(report_path, 'w') as f:
        f.write(f"# Repository Analysis Report: {repo_name}\n\n")
        f.write(f"*Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}*\n\n")
        f.write("## Repository Statistics\n\n")
        py_files = len(dependencies['python']['files'])
        py_packages = len(dependencies['python']['requirements'])
        f.write(f"### Python\n")
        f.write(f"- **Files:** {py_files}\n")
        f.write(f"- **External Packages:** {py_packages}\n")
        if py_packages > 0:
            f.write("\n**Top Python Dependencies:**\n\n")
            f.write("| Package | Version |\n")
            f.write("|---------|--------|\n")
            top_deps = list(dependencies['python']['requirements'].items())
            for package, version in sorted(top_deps)[:10]:  
                f.write(f"| {package} | {version} |\n")
        js_files = len(dependencies['javascript']['files'])
        js_packages = len(dependencies['javascript']['package_json'])
        f.write(f"\n### JavaScript\n")
        f.write(f"- **Files:** {js_files}\n")
        f.write(f"- **External Packages:** {js_packages}\n")
        if js_packages > 0:
            f.write("\n**Top JavaScript Dependencies:**\n\n")
            f.write("| Package | Version |\n")
            f.write("|---------|--------|\n")
            top_deps = list(dependencies['javascript']['package_json'].items())
            for package, version in sorted(top_deps)[:10]:  
                f.write(f"| {package} | {version} |\n")
        f.write("\n## Dependency Graph Analysis\n\n")
        f.write(f"- **Total Nodes:** {graph_analysis['stats']['total_nodes']}\n")
        f.write(f"- **Total Edges:** {graph_analysis['stats']['total_edges']}\n")
        f.write(f"- **Graph Density:** {graph_analysis['density']:.4f}\n")
        f.write(f"- **Average Clustering:** {graph_analysis['avg_clustering']:.4f}\n")
        f.write(f"- **Number of Clusters:** {graph_analysis['clusters']}\n")
        if 'central_nodes' in graph_analysis and graph_analysis['central_nodes']:
            f.write("\n### Most Central Components\n\n")
            if 'degree' in graph_analysis['central_nodes']:
                f.write("**Most Connected Components (Degree Centrality):**\n\n")
                for node, centrality in graph_analysis['central_nodes']['degree']:
                    f.write(f"- {node} ({centrality:.4f})\n")
            if 'pagerank' in graph_analysis['central_nodes']:
                f.write("\n**Most Important Components (PageRank):**\n\n")
                for node, centrality in graph_analysis['central_nodes']['pagerank']:
                    f.write(f"- {node} ({centrality:.4f})\n")
        if 'cycles' in graph_analysis and graph_analysis['cycles']:
            f.write("\n### Circular Dependencies\n\n")
            f.write("The following circular dependencies were detected:\n\n")
            for cycle in graph_analysis['cycles']:
                f.write(f"- {' → '.join(cycle)} → {cycle[0]}\n")
        if 'recommendations' in graph_analysis and graph_analysis['recommendations']:
            f.write("\n### Graph Structure Recommendations\n\n")
            for rec in graph_analysis['recommendations']:
                f.write(f"- {rec}\n")
        if codebert_results:
            f.write("\n## Code Quality Analysis\n\n")
            if 'most_complex_files' in codebert_results and codebert_results['most_complex_files']:
                f.write("### Most Complex Files\n\n")
                for file in codebert_results['most_complex_files']:
                    f.write(f"- `{file}`\n")
            if 'file_analysis' in codebert_results and codebert_results['file_analysis']:
                f.write("\n### Code Quality Metrics\n\n")
                f.write("| File | Language | Lines | Comment Ratio | Quality Score |\n")
                f.write("|------|----------|-------|---------------|---------------|\n")
                files_by_quality = sorted(
                    codebert_results['file_analysis'].items(),
                    key=lambda x: x[1]['quality_score']
                )
                for file, data in files_by_quality[:10]:  
                    f.write(f"| `{file}` | {data['language']} | {data['lines_of_code']} | {data['comment_ratio']:.2f} | {data['quality_score']:.2f} |\n")
                f.write("\n...\n\n")
                for file, data in files_by_quality[-10:]:  
                    f.write(f"| `{file}` | {data['language']} | {data['lines_of_code']} | {data['comment_ratio']:.2f} | {data['quality_score']:.2f} |\n")
            if 'overall_suggestions' in codebert_results and codebert_results['overall_suggestions']:
                f.write("\n### Overall Code Quality Suggestions\n\n")
                for suggestion in codebert_results['overall_suggestions']:
                    f.write(f"- {suggestion}\n")
            sample_count = 0
            if 'file_analysis' in codebert_results:
                f.write("\n### Sample File-Specific Suggestions\n\n")
                for file, data in codebert_results['file_analysis'].items():
                    if 'suggestions' in data and data['suggestions'] and sample_count < 5:
                        f.write(f"**File: `{file}`**\n\n")
                        for suggestion in data['suggestions'][:3]:  
                            f.write(f"- {suggestion}\n")
                        f.write("\n")
                        sample_count += 1
        f.write("\n## Conclusion\n\n")
        f.write("This report provides an overview of the repository structure, dependencies, and code quality. ")
        f.write("Use the interactive visualization for a more detailed exploration of the dependency relationships.\n\n")
        f.write("To improve the codebase, focus on addressing circular dependencies, refactoring complex files, and following the provided recommendations.")
        logger.info(f"Report generated at {report_path}")

---CODE_SEPARATOR---

def __init__(self, graph, repo_name):
        self.graph = graph
        self.repo_name = repo_name
    def create_network_visualization(self, output_path="dependency_graph.html"):
        try:
            logger.info("Creating interactive dependency graph visualization...")
            net = Network(height="750px", width="100%", directed=True, notebook=False)
            net.toggle_physics(True)
            net.set_options()
            node_colors = {
                "python_file": "#3572A5",      
                "js_file": "#F7DF1E",          
                "python_package": "#FFD43B",   
                "js_package": "#F0DB4F"        
            }
            for node, attr in self.graph.nodes(data=True):
                size = 15
                if attr.get('type') in ['python_package', 'js_package']:
                    size = 10
                if self.graph.degree(node) > 5:
                    size += 5
                net.add_node(
                    node, 
                    label=attr.get('label', node), 
                    title=attr.get('title', node),
                    color=node_colors.get(attr.get('type'), "#CCCCCC"), 
                    size=size
                )
            for source, target, attr in self.graph.edges(data=True):
                color = "#cccccc"
                if attr.get('type') == 'internal_import':
                    color = "#007bff"  
                elif attr.get('type') == 'external_import':
                    color = "#28a745"  
                net.add_edge(source, target, color=color, arrows='to')
            metadata = {
                "Repository": self.repo_name,
                "Python Files": self.graph.graph.get('python_files', 0),
                "JavaScript Files": self.graph.graph.get('js_files', 0),
                "Python Packages": self.graph.graph.get('python_packages', 0),
                "JavaScript Packages": self.graph.graph.get('js_packages', 0),
                "Total Nodes": self.graph.number_of_nodes(),
                "Total Edges": self.graph.number_of_edges()
            }
            html_template = f
            for key, value in metadata.items():
                html_template += f'<li class="list-group-item d-flex justify-content-between align-items-center">{key} <span class="badge bg-primary rounded-pill">{value}</span></li>\n'
            html_template += 
            net.save_graph(output_path)
            with open(output_path, 'r', encoding='utf-8') as f:
                pyvis_html = f.read()
            script_start = pyvis_html.find('<script type="text/javascript">')
            script_end = pyvis_html.find('</script>', script_start) + len('</script>')
            vis_script = pyvis_html[script_start:script_end]
            vis_script_fixed = 
            nodes_json = json.dumps([
                {
                    'id': node,
                    'label': self.graph.nodes[node].get('label', node),
                    'title': self.graph.nodes[node].get('title', node),
                    'color': node_colors.get(self.graph.nodes[node].get('type'), "#CCCCCC"),
                    'shape': 'dot',
                    'size': 10 if self.graph.nodes[node].get('type') in ['python_package', 'js_package'] else 15
                }
                for node in self.graph.nodes()
            ])
            edges_json = json.dumps([
                {
                    'from': source,
                    'to': target,
                    'color': {
                        'color': "#007bff" if self.graph.edges[source, target].get('type') == 'internal_import'
                              else "#28a745" if self.graph.edges[source, target].get('type') == 'external_import'
                              else "#cccccc"
                    },
                    'arrows': 'to'
                }
                for source, target in self.graph.edges()
            ])
            vis_script_fixed = vis_script_fixed.replace('DATA_NODES_PLACEHOLDER', nodes_json)
            vis_script_fixed = vis_script_fixed.replace('DATA_EDGES_PLACEHOLDER', edges_json)
            complete_html = html_template + vis_script_fixed + 
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(complete_html)
            logger.info(f"Interactive visualization saved to {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"Error creating visualization: {e}")
            error_html = f
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(error_html)
            raise
    def create_static_graph(self, output_path="dependency_graph.png"):
        try:
            logger.info("Creating static dependency graph visualization...")
            viz_graph = self.graph.copy()
            node_colors = []
            for node in viz_graph.nodes():
                node_type = viz_graph.nodes[node].get('type')
                if node_type == 'python_file':
                    node_colors.append('#3572A5')  
                elif node_type == 'js_file':
                    node_colors.append('#F7DF1E')  
                elif node_type == 'python_package':
                    node_colors.append('#FFD43B')  
                elif node_type == 'js_package':
                    node_colors.append('#F0DB4F')  
                else:
                    node_colors.append('#CCCCCC')  
            plt.figure(figsize=(14, 10))
            plt.title(f"Dependency Graph: {self.repo_name}", fontsize=16)
            if viz_graph.number_of_nodes() > 0:
                try:
                    pos = nx.spring_layout(viz_graph, k=0.15, iterations=50)
                    nx.draw_networkx_nodes(viz_graph, pos, node_size=300, node_color=node_colors, alpha=0.8)
                    nx.draw_networkx_edges(viz_graph, pos, edge_color='#CCCCCC', arrows=True, alpha=0.5)
                    labels = {}
                    for node in viz_graph.nodes():
                        if viz_graph.degree(node) > 3 or viz_graph.nodes[node].get('type') in ['python_package', 'js_package']:
                            labels[node] = viz_graph.nodes[node].get('label', node)
                    nx.draw_networkx_labels(viz_graph, pos, labels=labels, font_size=8)
                    legend_elements = [
                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#3572A5', markersize=10, label='Python File'),
                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#F7DF1E', markersize=10, label='JavaScript File'),
                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FFD43B', markersize=10, label='Python Package'),
                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#F0DB4F', markersize=10, label='JS Package')
                    ]
                    plt.legend(handles=legend_elements, loc='upper right')
                    plt.figtext(0.02, 0.02, f"Repository: {self.repo_name}\n"
                               f"Files: Python: {self.graph.graph.get('python_files', 0)}, "
                               f"JavaScript: {self.graph.graph.get('js_files', 0)}\n"
                               f"Packages: Python: {self.graph.graph.get('python_packages', 0)}, "
                               f"JavaScript: {self.graph.graph.get('js_packages', 0)}", 
                               fontsize=10)
                except Exception as layout_error:
                    logger.warning(f"Error in graph layout: {layout_error}. Using random layout instead.")
                    pos = nx.random_layout(viz_graph)
                    nx.draw_networkx(viz_graph, pos, node_color=node_colors, 
                                    edge_color='#CCCCCC', arrows=True, 
                                    node_size=300, font_size=8, alpha=0.8)
                    plt.text(0.5, 0.95, "Warning: Using random layout due to layout calculation error", 
                            ha='center', va='center', fontsize=10, color='red',
                            transform=plt.gca().transAxes)
            else:
                plt.text(0.5, 0.5, "No dependencies found", ha='center', va='center', fontsize=14)
            plt.axis('off')
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()
            logger.info(f"Static visualization saved to {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"Error creating static visualization: {e}")
            try:
                plt.figure(figsize=(8, 6))
                plt.text(0.5, 0.5, f"Error creating visualization:\n{str(e)}", 
                        ha='center', va='center', fontsize=12, color='red')
                plt.axis('off')
                plt.savefig(output_path, dpi=100)
                plt.close()
                logger.info(f"Error image saved to {output_path}")
            except Exception as error_img_error:
                logger.error(f"Could not create error image: {error_img_error}")
            raise
    def export_graph_data(self, output_path="dependency_data.json"):
        try:
            logger.info("Exporting graph data to JSON...")
            data = {
                "repo_name": self.repo_name,
                "metadata": dict(self.graph.graph),
                "nodes": [],
                "edges": []
            }
            for node, attrs in self.graph.nodes(data=True):
                node_data = {"id": node}
                serializable_attrs = {}
                for key, value in attrs.items():
                    try:
                        json.dumps({key: value})
                        serializable_attrs[key] = value
                    except (TypeError, OverflowError):
                        serializable_attrs[key] = str(value)
                node_data.update(serializable_attrs)
                data["nodes"].append(node_data)
            for source, target, attrs in self.graph.edges(data=True):
                edge_data = {"source": source, "target": target}
                serializable_attrs = {}
                for key, value in attrs.items():
                    try:
                        json.dumps({key: value})
                        serializable_attrs[key] = value
                    except (TypeError, OverflowError):
                        serializable_attrs[key] = str(value)
                edge_data.update(serializable_attrs)
                data["edges"].append(edge_data)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2)
            logger.info(f"Graph data exported to {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"Error exporting graph data: {e}")
            raise
    def safe_remove_directory(self, directory):
        try:
            import shutil
            logger.info(f"Attempting to remove directory: {directory}")
            shutil.rmtree(directory)
            logger.info(f"Successfully removed directory: {directory}")
        except PermissionError as e:
            logger.warning(f"Permission error when removing {directory}: {e}")
            try:
                import os
                import platform
                import time
                time.sleep(1)
                if platform.system() == 'Windows':
                    logger.info("Using Windows command to force directory removal")
                    os.system(f'rd /s /q "{directory}"')
                else:
                    logger.info("Using Unix command to force directory removal")
                    os.system(f'rm -rf "{directory}"')
                if not os.path.exists(directory):
                    logger.info(f"Directory successfully removed using system command: {directory}")
                else:
                    logger.warning(f"Failed to remove directory: {directory}")
            except Exception as inner_e:
                logger.error(f"Failed to remove directory {directory} using alternative method: {inner_e}")
        except Exception as e:
            logger.error(f"Error removing directory {directory}: {e}")

---CODE_SEPARATOR---

class DependencyVisualizer:
    def __init__(self, graph, repo_name):
        self.graph = graph
        self.repo_name = repo_name
    def create_network_visualization(self, output_path="dependency_graph.html"):
        try:
            logger.info("Creating interactive dependency graph visualization...")
            net = Network(height="750px", width="100%", directed=True, notebook=False)
            net.toggle_physics(True)
            net.set_options()
            node_colors = {
                "python_file": "#3572A5",      
                "js_file": "#F7DF1E",          
                "python_package": "#FFD43B",   
                "js_package": "#F0DB4F"        
            }
            for node, attr in self.graph.nodes(data=True):
                size = 15
                if attr.get('type') in ['python_package', 'js_package']:
                    size = 10
                if self.graph.degree(node) > 5:
                    size += 5
                net.add_node(
                    node, 
                    label=attr.get('label', node), 
                    title=attr.get('title', node),
                    color=node_colors.get(attr.get('type'), "#CCCCCC"), 
                    size=size
                )
            for source, target, attr in self.graph.edges(data=True):
                color = "#cccccc"
                if attr.get('type') == 'internal_import':
                    color = "#007bff"  
                elif attr.get('type') == 'external_import':
                    color = "#28a745"  
                net.add_edge(source, target, color=color, arrows='to')
            metadata = {
                "Repository": self.repo_name,
                "Python Files": self.graph.graph.get('python_files', 0),
                "JavaScript Files": self.graph.graph.get('js_files', 0),
                "Python Packages": self.graph.graph.get('python_packages', 0),
                "JavaScript Packages": self.graph.graph.get('js_packages', 0),
                "Total Nodes": self.graph.number_of_nodes(),
                "Total Edges": self.graph.number_of_edges()
            }
            html_template = f
            for key, value in metadata.items():
                html_template += f'<li class="list-group-item d-flex justify-content-between align-items-center">{key} <span class="badge bg-primary rounded-pill">{value}</span></li>\n'
            html_template += 
            net.save_graph(output_path)
            with open(output_path, 'r', encoding='utf-8') as f:
                pyvis_html = f.read()
            script_start = pyvis_html.find('<script type="text/javascript">')
            script_end = pyvis_html.find('</script>', script_start) + len('</script>')
            vis_script = pyvis_html[script_start:script_end]
            vis_script_fixed = 
            nodes_json = json.dumps([
                {
                    'id': node,
                    'label': self.graph.nodes[node].get('label', node),
                    'title': self.graph.nodes[node].get('title', node),
                    'color': node_colors.get(self.graph.nodes[node].get('type'), "#CCCCCC"),
                    'shape': 'dot',
                    'size': 10 if self.graph.nodes[node].get('type') in ['python_package', 'js_package'] else 15
                }
                for node in self.graph.nodes()
            ])
            edges_json = json.dumps([
                {
                    'from': source,
                    'to': target,
                    'color': {
                        'color': "#007bff" if self.graph.edges[source, target].get('type') == 'internal_import'
                              else "#28a745" if self.graph.edges[source, target].get('type') == 'external_import'
                              else "#cccccc"
                    },
                    'arrows': 'to'
                }
                for source, target in self.graph.edges()
            ])
            vis_script_fixed = vis_script_fixed.replace('DATA_NODES_PLACEHOLDER', nodes_json)
            vis_script_fixed = vis_script_fixed.replace('DATA_EDGES_PLACEHOLDER', edges_json)
            complete_html = html_template + vis_script_fixed + 
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(complete_html)
            logger.info(f"Interactive visualization saved to {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"Error creating visualization: {e}")
            error_html = f
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(error_html)
            raise
    def create_static_graph(self, output_path="dependency_graph.png"):
        try:
            logger.info("Creating static dependency graph visualization...")
            viz_graph = self.graph.copy()
            node_colors = []
            for node in viz_graph.nodes():
                node_type = viz_graph.nodes[node].get('type')
                if node_type == 'python_file':
                    node_colors.append('#3572A5')  
                elif node_type == 'js_file':
                    node_colors.append('#F7DF1E')  
                elif node_type == 'python_package':
                    node_colors.append('#FFD43B')  
                elif node_type == 'js_package':
                    node_colors.append('#F0DB4F')  
                else:
                    node_colors.append('#CCCCCC')  
            plt.figure(figsize=(14, 10))
            plt.title(f"Dependency Graph: {self.repo_name}", fontsize=16)
            if viz_graph.number_of_nodes() > 0:
                try:
                    pos = nx.spring_layout(viz_graph, k=0.15, iterations=50)
                    nx.draw_networkx_nodes(viz_graph, pos, node_size=300, node_color=node_colors, alpha=0.8)
                    nx.draw_networkx_edges(viz_graph, pos, edge_color='#CCCCCC', arrows=True, alpha=0.5)
                    labels = {}
                    for node in viz_graph.nodes():
                        if viz_graph.degree(node) > 3 or viz_graph.nodes[node].get('type') in ['python_package', 'js_package']:
                            labels[node] = viz_graph.nodes[node].get('label', node)
                    nx.draw_networkx_labels(viz_graph, pos, labels=labels, font_size=8)
                    legend_elements = [
                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#3572A5', markersize=10, label='Python File'),
                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#F7DF1E', markersize=10, label='JavaScript File'),
                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#FFD43B', markersize=10, label='Python Package'),
                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='#F0DB4F', markersize=10, label='JS Package')
                    ]
                    plt.legend(handles=legend_elements, loc='upper right')
                    plt.figtext(0.02, 0.02, f"Repository: {self.repo_name}\n"
                               f"Files: Python: {self.graph.graph.get('python_files', 0)}, "
                               f"JavaScript: {self.graph.graph.get('js_files', 0)}\n"
                               f"Packages: Python: {self.graph.graph.get('python_packages', 0)}, "
                               f"JavaScript: {self.graph.graph.get('js_packages', 0)}", 
                               fontsize=10)
                except Exception as layout_error:
                    logger.warning(f"Error in graph layout: {layout_error}. Using random layout instead.")
                    pos = nx.random_layout(viz_graph)
                    nx.draw_networkx(viz_graph, pos, node_color=node_colors, 
                                    edge_color='#CCCCCC', arrows=True, 
                                    node_size=300, font_size=8, alpha=0.8)
                    plt.text(0.5, 0.95, "Warning: Using random layout due to layout calculation error", 
                            ha='center', va='center', fontsize=10, color='red',
                            transform=plt.gca().transAxes)
            else:
                plt.text(0.5, 0.5, "No dependencies found", ha='center', va='center', fontsize=14)
            plt.axis('off')
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()
            logger.info(f"Static visualization saved to {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"Error creating static visualization: {e}")
            try:
                plt.figure(figsize=(8, 6))
                plt.text(0.5, 0.5, f"Error creating visualization:\n{str(e)}", 
                        ha='center', va='center', fontsize=12, color='red')
                plt.axis('off')
                plt.savefig(output_path, dpi=100)
                plt.close()
                logger.info(f"Error image saved to {output_path}")
            except Exception as error_img_error:
                logger.error(f"Could not create error image: {error_img_error}")
            raise
    def export_graph_data(self, output_path="dependency_data.json"):
        try:
            logger.info("Exporting graph data to JSON...")
            data = {
                "repo_name": self.repo_name,
                "metadata": dict(self.graph.graph),
                "nodes": [],
                "edges": []
            }
            for node, attrs in self.graph.nodes(data=True):
                node_data = {"id": node}
                serializable_attrs = {}
                for key, value in attrs.items():
                    try:
                        json.dumps({key: value})
                        serializable_attrs[key] = value
                    except (TypeError, OverflowError):
                        serializable_attrs[key] = str(value)
                node_data.update(serializable_attrs)
                data["nodes"].append(node_data)
            for source, target, attrs in self.graph.edges(data=True):
                edge_data = {"source": source, "target": target}
                serializable_attrs = {}
                for key, value in attrs.items():
                    try:
                        json.dumps({key: value})
                        serializable_attrs[key] = value
                    except (TypeError, OverflowError):
                        serializable_attrs[key] = str(value)
                edge_data.update(serializable_attrs)
                data["edges"].append(edge_data)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2)
            logger.info(f"Graph data exported to {output_path}")
            return output_path
        except Exception as e:
            logger.error(f"Error exporting graph data: {e}")
            raise
    def safe_remove_directory(self, directory):
        try:
            import shutil
            logger.info(f"Attempting to remove directory: {directory}")
            shutil.rmtree(directory)
            logger.info(f"Successfully removed directory: {directory}")
        except PermissionError as e:
            logger.warning(f"Permission error when removing {directory}: {e}")
            try:
                import os
                import platform
                import time
                time.sleep(1)
                if platform.system() == 'Windows':
                    logger.info("Using Windows command to force directory removal")
                    os.system(f'rd /s /q "{directory}"')
                else:
                    logger.info("Using Unix command to force directory removal")
                    os.system(f'rm -rf "{directory}"')
                if not os.path.exists(directory):
                    logger.info(f"Directory successfully removed using system command: {directory}")
                else:
                    logger.warning(f"Failed to remove directory: {directory}")
            except Exception as inner_e:
                logger.error(f"Failed to remove directory {directory} using alternative method: {inner_e}")
        except Exception as e:
            logger.error(f"Error removing directory {directory}: {e}")

---CODE_SEPARATOR---

